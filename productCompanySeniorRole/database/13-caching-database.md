Excellent.
Now we move into **real-world performance engineering**.

At scale, DB alone is never enough.
This is where senior backend engineers shine.

---

# ğŸ”¥ STEP 13 â€” CACHING + DATABASE INTERACTION

(Performance Engineering Layer)

Goal:
Reduce DB load without breaking consistency.

---

# ğŸ§  1ï¸âƒ£ Why Caching Exists

Database is:

* Disk-based
* Limited connections
* Expensive for repetitive reads

Most production systems:

90% reads
10% writes

Instead of hitting DB every time â†’ use cache.

---

# ğŸ§± 2ï¸âƒ£ Typical Architecture

Client
â†’ App Server
â†’ Cache (Redis)
â†’ Database

---

# ğŸ§  3ï¸âƒ£ Cache-Aside Pattern (Most Common)

Flow:

1. App checks cache
2. If present â†’ return
3. If not â†’ fetch from DB
4. Store in cache

Example (pseudo):

```java
User user = redis.get("user:101");

if (user == null) {
    user = db.getUser(101);
    redis.set("user:101", user);
}
```

This is cache-aside.

---

# ğŸ§  4ï¸âƒ£ Write Handling (Critical)

Case: user updates profile.

Flow:

```sql
UPDATE users
SET name = 'Rahul'
WHERE id = 101;
```

Then:

Invalidate cache:

```
redis.delete("user:101");
```

Next read â†’ fresh DB â†’ cache again.

This avoids stale data.

---

# ğŸ§  5ï¸âƒ£ Cache Invalidation Problem

Hardest problem in CS.

If you forget to invalidate:

Cache becomes stale.

Solution options:

* Time-to-live (TTL)
* Explicit invalidation
* Versioning

---

# ğŸ§  6ï¸âƒ£ TTL Strategy

Set expiration:

```
redis.set("user:101", data, TTL=300 seconds);
```

After 5 minutes â†’ auto-expire.

Tradeoff:
Possible stale data within TTL window.

---

# ğŸ§  7ï¸âƒ£ Write-Through Cache

Instead of:

Write to DB then invalidate.

You:

1. Write to cache
2. Cache writes to DB

Ensures consistency.

More complex.

---

# ğŸ§  8ï¸âƒ£ Write-Back (Dangerous)

Write only to cache first.
DB updated later asynchronously.

Fast but risky.

Used in high-throughput systems.

---

# ğŸ§  9ï¸âƒ£ Cache Stampede Problem

If key expires:

1000 requests hit DB simultaneously.

DB overload.

Solution:

* Mutex lock
* Request coalescing
* Staggered TTL
* Pre-warming

Senior must mention this.

---

# ğŸ§  10ï¸âƒ£ Hot Key Problem

If one key extremely popular:

All traffic hits one Redis node.

Solution:

* Shard cache
* Replicate hot key
* Use local in-memory cache

---

# ğŸ§  11ï¸âƒ£ When NOT to Cache

Donâ€™t cache:

* Frequently updated data
* Low reuse queries
* Huge result sets

Cache is not magic.

---

# ğŸ§  12ï¸âƒ£ Read-Through vs Cache-Aside

Cache-aside:
App manages logic.

Read-through:
Cache layer handles DB fetch automatically.

Most systems use cache-aside.

---

# ğŸ§  13ï¸âƒ£ Query-Level Caching Example

Heavy query:

```sql
SELECT SUM(amount)
FROM orders
WHERE created_at > NOW() - INTERVAL 1 DAY;
```

Instead of running repeatedly:

Cache result for 1 minute.

---

# ğŸ§  14ï¸âƒ£ Consistency Tradeoff

Strong consistency:
Always hit DB.

High performance:
Use cache.

Senior answer:
Depends on business criticality.

Payment ledger â†’ avoid stale cache.

User profile â†’ cache OK.

---

# ğŸ¯ HOW TO ANSWER IN INTERVIEW

If asked:

â€œDB under heavy read load. What will you do?â€

You say:

1. Add read replicas
2. Introduce Redis cache
3. Cache hot queries
4. Handle invalidation
5. Monitor hit ratio

Structured answer.

---

# ğŸ¯ STEP 13 CHECKPOINT

You must be able to:

âœ” Explain cache-aside
âœ” Handle invalidation
âœ” Explain cache stampede
âœ” Explain hot key
âœ” Explain TTL tradeoff

---

# ğŸ§  MINI TEST

1. Why caching can cause stale reads?
2. What is cache stampede?
3. Why write-back risky?
4. Why not cache everything?

If you answer calmly â†’ strong.

---

Next:

ğŸ”¥ STEP 14 â€” FAILURE HANDLING + RETRY + IDEMPOTENCY
(This is fintech-level maturity)

Type **14**.
