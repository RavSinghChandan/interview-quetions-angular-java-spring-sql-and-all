Good. Now we go into resilience layer.

---

# ğŸ“ File: `12-container-crash-handling-self-healing.md`

# ğŸ”¥ STEP 12 â€” Container Crash Handling + Restart Policies + Self-Healing

(Production Resilience â€” Senior Backend Maturity)

This file is about failure.

Interviewers love asking:

* What happens if a container crashes?
* What is CrashLoopBackOff?
* How does Kubernetes self-heal?
* What would you check if pods keep restarting?

This is operational depth.

---

# ğŸ§  1ï¸âƒ£ Container Crash â€” What Actually Happens?

If application throws fatal exception:

```java
System.exit(1);
```

Or unhandled runtime exception â†’
Main process exits â†’ Container stops.

Docker behavior depends on restart policy.

---

# ğŸ§  2ï¸âƒ£ Docker Restart Policies

Run container:

```bash
docker run --restart=always myapp
```

Options:

* no (default)
* on-failure
* always
* unless-stopped

Example:

```bash
docker run --restart=on-failure:3 myapp
```

Retries up to 3 times.

In production â†’ Kubernetes handles restart.

---

# ğŸ§  3ï¸âƒ£ Kubernetes Self-Healing Mechanism

If container crashes:

1. Kubelet detects container exit
2. Pod restarted automatically
3. If repeated failures â†’ CrashLoopBackOff

Self-healing built-in.

---

# ğŸ§  4ï¸âƒ£ What Is CrashLoopBackOff?

Meaning:

* Pod started
* Crashed
* Restarted
* Crashed again
* Exponential backoff applied

Check:

```bash
kubectl get pods
```

Status:

```
CrashLoopBackOff
```

---

# ğŸ§  5ï¸âƒ£ Debugging CrashLoopBackOff

Check logs:

```bash
kubectl logs mypod
```

If restarted quickly:

```bash
kubectl logs mypod --previous
```

Common causes:

* Missing environment variable
* DB connection failure
* Wrong port
* OOMKilled
* Startup failure

---

# ğŸ§  6ï¸âƒ£ Liveness Probe Interaction

If liveness probe fails repeatedly:

Kubernetes restarts pod.

Bad health check configuration â†’ restart loop.

Be careful.

---

# ğŸ§  7ï¸âƒ£ OOMKilled vs CrashLoopBackOff

OOMKilled:

Memory exceeded â†’ kernel kill.

CrashLoopBackOff:

Application keeps failing to start.

Check:

```bash
kubectl describe pod mypod
```

Look under:

```
Last State
Reason
```

---

# ğŸ§  8ï¸âƒ£ Backoff Mechanism

Kubernetes increases delay between restarts:

Restart after:

* 10s
* 20s
* 40s
* 80s

Prevents infinite restart storms.

---

# ğŸ§  9ï¸âƒ£ Pod Restart Policy

In Kubernetes:

```yaml
restartPolicy: Always
```

Options:

* Always (default for Deployment)
* OnFailure
* Never (for Jobs)

Deployment pods â†’ Always.

---

# ğŸ§  ğŸ”Ÿ Production Resilience Strategy

Good design:

âœ” Idempotent startup
âœ” Retry logic for DB
âœ” Retry external API
âœ” Proper health checks
âœ” Circuit breaker pattern

App must tolerate temporary failures.

---

# ğŸ§  1ï¸âƒ£1ï¸âƒ£ Startup Dependency Handling

Common mistake:

App fails immediately if DB unavailable.

Better:

Retry with backoff.

Example (pseudo-code):

```java
for (int i = 0; i < 5; i++) {
    try {
        connectToDatabase();
        break;
    } catch (Exception e) {
        Thread.sleep(2000);
    }
}
```

Better approach:

Use Spring retry mechanism.

---

# ğŸ§  1ï¸âƒ£2ï¸âƒ£ Readiness vs Crash

If DB temporarily down:

Better behavior:

* Readiness fails
* Traffic removed
* Container not restarted

Not everything requires restart.

Restart is expensive.

---

# ğŸ§  1ï¸âƒ£3ï¸âƒ£ Observability During Crash

Check:

```bash
kubectl describe pod mypod
kubectl logs mypod
kubectl get events
```

Look for:

* OOMKilled
* Probe failure
* Image pull error
* Mount failure

---

# ğŸ§  1ï¸âƒ£4ï¸âƒ£ Real Interview Scenarios

---

### Scenario 1

Pod stuck in CrashLoopBackOff.

What do you check?

Answer structure:

* Logs
* Resource limits
* Env variables
* Health probe config
* Image correctness

---

### Scenario 2

Pod running but not serving traffic.

Likely:

* Readiness failing
* Service selector mismatch
* Port mismatch

---

### Scenario 3

Pods restarting under load.

Possible:

* OOMKilled
* CPU throttling
* DB timeout
* External dependency crash

---

# ğŸ§  1ï¸âƒ£5ï¸âƒ£ Senior-Level Explanation Structure

If asked:

â€œHow does Kubernetes ensure high availability?â€

Answer:

* Restart failed containers automatically
* Reschedule pods if node fails
* Use readiness probes to avoid routing traffic to unhealthy pods
* Rolling updates prevent downtime
* Horizontal autoscaling handles load

Clear. Structured. Mature.

---

# ğŸ¯ STEP 12 REVISION CHECKLIST

You must confidently explain:

âœ” Docker restart policies
âœ” Kubernetes self-healing
âœ” CrashLoopBackOff
âœ” OOMKilled vs crash
âœ” Debugging pods
âœ” Restart backoff logic
âœ” Readiness vs restart distinction
âœ” Resilience best practices

If you can explain these clearly â†’
Production resilience mastery achieved.

---

When ready, type:

**13**

Next file:

ğŸ“ `13-scaling-autoscaling-capacity-planning.md`
(Horizontal scaling + HPA + CPU-based scaling + memory-based scaling + scaling pitfalls)
