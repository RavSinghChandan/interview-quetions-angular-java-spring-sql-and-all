Good. Stay sharp.
Now we enter performance depth.

---

# ğŸ“ File: `09-resource-limits-jvm-tuning-performance.md`

# ğŸ”¥ STEP 9 â€” Resource Limits + OOMKilled + CPU Throttling + JVM Tuning

(Senior Backend Performance Awareness in Containers)

This is where strong backend engineers stand out.

Interviewers ask:

* What is OOMKilled?
* Why does JVM crash in container?
* How do CPU limits affect latency?
* How do you debug memory leak in Kubernetes?

This file makes you production-ready.

---

# ğŸ§  1ï¸âƒ£ Resource Limits in Containers

Containers are limited via **cgroups**.

You define limits at runtime:

### Docker:

```bash
docker run --memory="512m" --cpus="1.0" myapp
```

### Kubernetes:

```yaml
resources:
  requests:
    memory: "256Mi"
    cpu: "500m"
  limits:
    memory: "512Mi"
    cpu: "1"
```

Difference:

* Requests â†’ guaranteed minimum
* Limits â†’ maximum allowed

---

# ğŸ§  2ï¸âƒ£ What Is OOMKilled?

OOM = Out Of Memory.

If container memory usage exceeds limit:

Linux kernel kills process.

Check:

```bash
docker inspect <container_id>
```

Look for:

```json
"OOMKilled": true
```

In Kubernetes:

```bash
kubectl describe pod mypod
```

Look for:

```
Reason: OOMKilled
```

Very common real-world issue.

---

# ğŸ§  3ï¸âƒ£ Why JVM Gets OOMKilled in Container

Scenario:

Container memory limit = 512MB
JVM default heap = 1GB

JVM tries to allocate >512MB â†’
Kernel kills container.

This is NOT Java OOM.
This is Linux OOM kill.

Important distinction.

---

# ğŸ§  4ï¸âƒ£ JVM Heap Tuning for Containers

Correct practice:

Heap must be less than container limit.

Example:

Container = 512MB

Set:

```dockerfile
ENTRYPOINT ["java","-Xms256m","-Xmx384m","-jar","app.jar"]
```

Keep headroom for:

* Metaspace
* Thread stacks
* Native memory
* GC overhead

Rule of thumb:

Heap â‰ˆ 60â€“70% of container memory.

---

# ğŸ§  5ï¸âƒ£ Modern JVM Container Awareness

Java 10+ supports container memory detection.

Option:

```bash
-XX:+UseContainerSupport
```

Now JVM detects cgroup limits.

Still recommended to explicitly control heap.

---

# ğŸ§  6ï¸âƒ£ CPU Limits and Throttling

If set:

```bash
docker run --cpus="0.5" myapp
```

App gets half CPU core.

If traffic high:

* CPU throttling occurs
* Response time increases
* GC pauses longer

In Kubernetes:

```yaml
limits:
  cpu: "500m"
```

500m = 0.5 CPU.

Throttling is real. It impacts latency.

---

# ğŸ§  7ï¸âƒ£ Requests vs Limits (Kubernetes Critical Concept)

Example:

```yaml
requests:
  cpu: "250m"
limits:
  cpu: "1"
```

Meaning:

* Scheduler reserves 0.25 CPU
* Pod allowed up to 1 CPU

If you set limit too low â†’ throttling.

If you set no limit â†’ may starve other pods.

Senior balance required.

---

# ğŸ§  8ï¸âƒ£ Monitoring Resource Usage

Docker:

```bash
docker stats
```

Kubernetes:

```bash
kubectl top pod mypod
```

You must correlate:

* CPU spikes
* Memory spikes
* GC pauses
* Traffic patterns

---

# ğŸ§  9ï¸âƒ£ Debugging High Memory Usage

Steps:

1. Check container limit
2. Check JVM heap size
3. Check GC logs
4. Check object retention (heap dump)
5. Check connection pool size
6. Check caching layer

Heap dump inside container:

```bash
jmap -dump:live,format=b,file=heap.hprof <pid>
```

Advanced but strong signal.

---

# ğŸ§  ğŸ”Ÿ Debugging CPU Spikes

Possible causes:

* Infinite loop
* Blocking I/O
* Thread starvation
* Too many threads
* GC overhead

Check thread dump:

```bash
jstack <pid>
```

Inside container:

```bash
docker exec -it myapp jstack 1
```

---

# ğŸ§  1ï¸âƒ£1ï¸âƒ£ GC Behavior in Containers

If heap too large:

* Long GC pauses

If heap too small:

* Frequent GC
* High CPU usage

Balance is required.

Common JVM flags:

```bash
-XX:+UseG1GC
```

G1GC is default in modern JVM.

---

# ğŸ§  1ï¸âƒ£2ï¸âƒ£ Memory Leak in Kubernetes Scenario

Symptoms:

* Memory gradually increases
* Eventually OOMKilled
* Pod restarts repeatedly

Investigate:

* Heap dump
* Caching strategy
* Unclosed resources
* Static collections

---

# ğŸ§  1ï¸âƒ£3ï¸âƒ£ Real Interview Scenarios

---

### Scenario 1 â€” Pod keeps restarting every few hours

Likely:

* Memory leak
* OOMKilled
* GC misconfiguration

---

### Scenario 2 â€” Latency increases under load

Possible:

* CPU throttling
* DB bottleneck
* Thread pool exhaustion
* GC pressure

---

### Scenario 3 â€” Works locally, fails in container

Likely:

* Local machine unlimited memory
* Container limited
* JVM not tuned

---

# ğŸ§  1ï¸âƒ£4ï¸âƒ£ Production Resource Strategy

Good practice:

âœ” Define requests and limits
âœ” Tune JVM heap properly
âœ” Monitor metrics
âœ” Use autoscaling
âœ” Avoid unlimited memory

---

# ğŸ§  1ï¸âƒ£5ï¸âƒ£ Senior-Level Explanation Structure

If asked:

â€œHow do you handle memory management in Kubernetes?â€

Answer:

* Define memory requests and limits
* Tune JVM heap relative to limit
* Monitor OOMKilled events
* Analyze heap dump if leak suspected
* Use autoscaling for traffic spikes

Clear. Structured. Confident.

---

# ğŸ¯ STEP 9 REVISION CHECKLIST

You must confidently explain:

âœ” cgroups resource limits
âœ” OOMKilled vs Java OOM
âœ” JVM heap tuning in container
âœ” CPU throttling
âœ” requests vs limits
âœ” docker stats / kubectl top
âœ” GC impact
âœ” Debugging memory leak
âœ” Thread dump usage

If you can explain all clearly â†’
Performance maturity achieved.

---

When ready, type:

**10**

Next file:

ğŸ“ `10-networking-service-communication.md`
(Container networking + bridge + service discovery + internal DNS + load balancing basics)
