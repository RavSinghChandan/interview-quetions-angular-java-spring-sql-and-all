Perfect.
Now we enter **real cluster resilience thinking**.

This is orchestration maturity beyond pods.

---

# ğŸ“ File: `12-self-healing-node-failure-recovery.md`

# ğŸ”¥ STEP 12 â€” Node Failure + Self-Healing + Cluster Recovery

(Control Plane Resilience Mechanics â€” Senior Level)

Interviewers may ask:

* What happens if a node crashes?
* How does Kubernetes detect node failure?
* How are pods rescheduled?
* What happens to running traffic?
* How does cluster remain available?

You must answer step-by-step.

---

# ğŸ§  1ï¸âƒ£ The Real Production Scenario

Imagine:

Node-1 running:

* 4 pods of your service

Suddenly:

* VM crashes
* Network failure
* Power loss

What happens?

---

# ğŸ§  2ï¸âƒ£ Node Heartbeat Mechanism

Each node sends heartbeat to API Server via Kubelet.

Heartbeat interval ~10 seconds.

If API server does not receive heartbeat:

Node marked:

```
NotReady
```

Check:

```bash
kubectl get nodes
```

Status becomes:

```
NotReady
```

---

# ğŸ§  3ï¸âƒ£ Node Controller Reaction

Node Controller in Control Plane detects:

Node missing heartbeat beyond threshold.

Actions:

1. Marks node NotReady
2. Marks pods on that node as Unknown
3. After grace period â†’ deletes pods

This triggers rescheduling.

---

# ğŸ§  4ï¸âƒ£ Pod Rescheduling Flow

Example:

Deployment replicas = 5
Node-1 dies â†’ 2 pods lost

Controller sees:

Desired = 5
Actual running = 3

Action:

Create 2 new pods.

Scheduler assigns them to healthy nodes.

Self-healing at cluster level.

---

# ğŸ§  5ï¸âƒ£ Traffic Impact During Node Failure

What happens to traffic?

Pods on failed node:

* Become unreachable
* Removed from Service endpoints

Service routes traffic only to healthy pods.

Users may see small spike in latency,
but system continues functioning.

---

# ğŸ§  6ï¸âƒ£ Time Delay in Recovery

Important:

Node failure detection not instant.

Timeline:

* Heartbeat missed
* Node marked NotReady (~40s default)
* Pods terminated
* New pods scheduled
* Containers start
* Readiness passes
* Traffic resumes

Recovery takes time.

Design for redundancy.

---

# ğŸ§  7ï¸âƒ£ Replica Importance

If replicas = 1

Node failure = total outage.

If replicas â‰¥ 2 across different nodes

System survives node failure.

High availability requires multiple replicas.

---

# ğŸ§  8ï¸âƒ£ PodDisruptionBudget (PDB)

Protect availability during voluntary disruptions.

Example:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
```

Ensures at least 2 pods always available.

Prevents too many pods being disrupted at once.

---

# ğŸ§  9ï¸âƒ£ Multi-Zone Deployment

In cloud:

Deploy across availability zones.

If one zone fails:

Pods in other zones continue.

Use:

```yaml
topologySpreadConstraints
```

Or node affinity.

Senior-level HA design.

---

# ğŸ§  ğŸ”Ÿ Cluster Autoscaler Interaction

If new pods cannot schedule due to lack of nodes:

Cluster Autoscaler:

* Adds new nodes automatically
* Scheduler assigns pending pods

Scaling at infrastructure level.

---

# ğŸ§  1ï¸âƒ£1ï¸âƒ£ What If etcd Fails?

If etcd fails:

Cluster state unavailable.

Control plane becomes unstable.

Production clusters:

* Run etcd in HA mode
* Multiple replicas

Control plane HA critical.

---

# ğŸ§  1ï¸âƒ£2ï¸âƒ£ Control Plane HA

Production setup:

Multiple API servers
Multiple controller managers
Multiple scheduler instances

Behind load balancer.

Ensures control plane availability.

---

# ğŸ§  1ï¸âƒ£3ï¸âƒ£ Real Interview Scenario

Question:

â€œWhat happens if a node crashes?â€

Strong answer:

* Kubelet heartbeat stops
* Node marked NotReady
* Node controller deletes pods
* ReplicaSet creates replacement pods
* Scheduler assigns to healthy nodes
* Service removes failed endpoints
* Traffic continues via remaining pods

Clear. Structured. Mature.

---

# ğŸ§  1ï¸âƒ£4ï¸âƒ£ Real Failure Scenario

Scenario:

Pods not rescheduling after node failure.

Possible causes:

* No available node capacity
* Taints preventing scheduling
* Resource requests too high
* Node autoscaler disabled

Debug:

```bash
kubectl describe pod mypod
```

---

# ğŸ§  1ï¸âƒ£5ï¸âƒ£ Senior-Level Mental Model

Kubernetes self-healing works at two layers:

Pod-level (container crash)
Node-level (infrastructure failure)

Control plane constantly reconciles desired state.

Cluster survives failure by:

Redundancy + reconciliation + rescheduling.

---

# ğŸ¯ STEP 12 REVISION CHECKLIST

You must confidently explain:

âœ” Node heartbeat detection
âœ” NotReady state
âœ” Pod rescheduling flow
âœ” Service endpoint update
âœ” Recovery timeline
âœ” Importance of replicas
âœ” PodDisruptionBudget
âœ” Multi-zone deployment
âœ” Control plane HA

If you can explain all clearly â†’
Cluster resilience mastery achieved.

---

When ready, type:

**13**

Next file:

ğŸ“ `13-statefulset-vs-deployment-storage-persistence.md`
(Stateful workloads + stable identity + persistent volumes + PVC + why DB needs StatefulSet deep dive)
